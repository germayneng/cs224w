{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **CS224W - Colab 2**","metadata":{"id":"XuXWJLEm2UWS"}},{"cell_type":"markdown","source":"In this Colab, we will construct our own graph neural network by using PyTorch Geometric (PyG) and apply the model on two of Open Graph Benchmark (OGB) datasets. Those two datasets are used to benchmark the model performance on two different graph-related tasks. One is node property prediction, predicting properties of single nodes. Another one is graph property prediction, predicting the entire graphs or subgraphs.\n\nAt first, we will learn how PyTorch Geometric stores the graphs in PyTorch tensor.\n\nWe will then load and take a quick look on one of the Open Graph Benchmark (OGB) datasets by using the `ogb` package. OGB is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. The `ogb` package not only provides the data loader of the dataset but also the evaluator.\n\nAt last, we will build our own graph neural networks by using PyTorch Geometric. And then apply and evaluate the models on node property prediction and grpah property prediction tasks.\n\n**Note**: Make sure to **sequentially run all the cells in each section**, so that the intermediate variables / packages will carry over to the next cell\n\nHave fun on Colab 2 :)","metadata":{"id":"8gzsP50bF6Gb"}},{"cell_type":"markdown","source":"# Device\nYou might need to use GPU for this Colab.\n\nPlease click `Runtime` and then `Change runtime type`. Then set the `hardware accelerator` to **GPU**.","metadata":{"id":"ZGKqVEbbMEzf"}},{"cell_type":"markdown","source":"# Installation","metadata":{"id":"B0NiFL6OLpaJ"}},{"cell_type":"code","source":"!nvcc --version","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2020 NVIDIA Corporation\nBuilt on Wed_Jul_22_19:09:09_PDT_2020\nCuda compilation tools, release 11.0, V11.0.221\nBuild cuda_11.0_bu.TC445_37.28845127_0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip list","metadata":{"scrolled":true,"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Package                        Version             Location\n------------------------------ ------------------- --------------\nabsl-py                        0.12.0\nadal                           1.2.6\naffine                         2.3.0\naiobotocore                    1.3.0\naiohttp                        3.7.4\naiohttp-cors                   0.7.0\naioitertools                   0.7.1\naioredis                       1.3.1\nalbumentations                 0.5.2\nalembic                        1.5.8\nallennlp                       2.3.0\naltair                         4.1.0\nannoy                          1.17.0\nansiwrap                       0.8.4\nappdirs                        1.4.4\nargon2-cffi                    20.1.0\narrow                          1.0.3\narviz                          0.11.2\nasn1crypto                     1.4.0\nastunparse                     1.6.3\nasync-generator                1.10\nasync-timeout                  3.0.1\nattrs                          20.3.0\naudioread                      2.1.9\nautocfg                        0.0.8\nautogluon.core                 0.1.0\nautograd                       1.3\nBabel                          2.9.0\nbackcall                       0.2.0\nbackports.functools-lru-cache  1.6.3\nbasemap                        1.2.1\nbayesian-optimization          1.2.0\nbayespy                        0.5.22\nbcrypt                         3.2.0\nbinaryornot                    0.4.4\nbiopython                      1.78\nblack                          20.8b1\nbleach                         3.3.0\nblessings                      1.7\nblinker                        1.4\nblis                           0.7.4\nbokeh                          2.3.1\nBoruta                         0.3\nboto3                          1.17.53\nbotocore                       1.20.53\nbq-helper                      0.4.1               /src/bq-helper\nbqplot                         0.12.25\nbranca                         0.4.2\nbrewer2mpl                     1.4.1\nbrotlipy                       0.7.0\ncachetools                     4.2.1\ncaip-notebooks-serverextension 1.0.0\nCartopy                        0.18.0\ncatalogue                      1.0.0\ncatalyst                       21.4.1\ncatboost                       0.25.1\ncategory-encoders              2.2.2\ncertifi                        2020.12.5\ncesium                         0.9.12\ncffi                           1.14.5\ncftime                         1.4.1\nchardet                        4.0.0\ncleverhans                     3.0.1\nclick                          7.1.2\nclick-plugins                  1.1.1\ncliff                          3.7.0\ncligj                          0.7.1\ncloud-tpu-client               0.10\ncloudpickle                    1.6.0\ncmaes                          0.8.2\ncmd2                           1.5.0\ncmdstanpy                      0.9.5\ncmudict                        0.4.5\ncolorama                       0.4.4\ncolorcet                       2.0.6\ncolorful                       0.5.4\ncolorlog                       5.0.1\ncolorlover                     0.3.0\nconda                          4.10.1\nconda-package-handling         1.7.2\nconfigparser                   5.0.2\nConfigSpace                    0.4.18\nconfuse                        1.4.0\ncontextily                     1.1.0\ncontextlib2                    0.6.0.post1\nconvertdate                    2.3.2\ncookiecutter                   1.7.2\ncryptography                   3.4.7\ncudf                           0.16.0\ncufflinks                      0.17.3\ncuml                           0.16.0\ncupy                           8.0.0\ncupy-cuda110                   8.6.0\nCVXcanon                       0.1.2\ncvxpy                          1.1.7\ncycler                         0.10.0\ncymem                          2.0.5\ncysignals                      1.10.3\nCython                         0.29.23\ncytoolz                        0.11.0\ndask                           2021.4.0\ndask-cudf                      0.16.0\ndataclasses                    0.6\ndatashader                     0.12.1\ndatashape                      0.5.2\ndatatable                      0.11.1\ndeap                           1.3.1\ndecorator                      4.4.2\ndecord                         0.5.2\ndeepdish                       0.3.6\ndefusedxml                     0.7.1\nDelorean                       1.0.0\nDeprecated                     1.2.12\ndescartes                      1.1.0\ndill                           0.3.3\ndipy                           1.4.0\ndistributed                    2021.4.0\ndlib                           19.22.0\ndm-tree                        0.1.6\ndocker                         4.4.4\ndocker-pycreds                 0.4.0\ndocutils                       0.17.1\nearthengine-api                0.1.261\neasydev                        0.11.0\neasyocr                        1.3.0.1\necos                           2.0.7.post1\neli5                           0.11.0\nemoji                          1.2.0\nen-core-web-lg                 2.3.1\nen-core-web-sm                 2.3.1\nentrypoints                    0.3\nephem                          3.7.7.1\nessentia                       2.1b6.dev374\nfancyimpute                    0.5.5\nfastai                         2.3.0\nfastavro                       1.4.0\nfastcore                       1.3.19\nfastprogress                   1.0.0\nfastrlock                      0.6\nfasttext                       0.9.2\nfbpca                          1.0\nfbprophet                      0.7.1\nfeather-format                 0.4.1\nfeaturetools                   0.23.3\nfilelock                       3.0.12\nFiona                          1.8.18\nfitter                         1.3.0\nflake8                         3.9.1\nflashtext                      2.7\nFlask                          1.1.2\nflatbuffers                    1.12\nfolium                         0.12.1\nfsspec                         0.8.7\nfuncy                          1.15\nfury                           0.7.0\nfuture                         0.18.2\nfuzzywuzzy                     0.18.0\ngast                           0.3.3\ngatspy                         0.3\ngcsfs                          0.7.2\nGDAL                           3.2.1\ngensim                         4.0.1\ngeographiclib                  1.50\nGeohash                        1.0\ngeojson                        2.5.0\ngeopandas                      0.9.0\ngeoplot                        0.4.1\ngeopy                          2.1.0\ngeoviews                       1.9.1\nggplot                         0.11.5\ngitdb                          4.0.7\nGitPython                      3.1.14\ngluoncv                        0.10.1.post0\ngluonnlp                       0.10.0\ngoogle-api-core                1.26.2\ngoogle-api-python-client       1.8.0\ngoogle-auth                    1.26.1\ngoogle-auth-httplib2           0.0.4\ngoogle-auth-oauthlib           0.4.3\ngoogle-cloud-aiplatform        0.6.0a1\ngoogle-cloud-automl            1.0.1\ngoogle-cloud-bigquery          2.2.0\ngoogle-cloud-bigtable          1.4.0\ngoogle-cloud-core              1.6.0\ngoogle-cloud-dataproc          1.1.1\ngoogle-cloud-datastore         1.12.0\ngoogle-cloud-firestore         1.8.1\ngoogle-cloud-kms               1.4.0\ngoogle-cloud-language          2.0.0\ngoogle-cloud-logging           1.15.1\ngoogle-cloud-monitoring        1.1.0\ngoogle-cloud-pubsub            1.7.0\ngoogle-cloud-scheduler         1.3.0\ngoogle-cloud-spanner           1.17.1\ngoogle-cloud-speech            1.3.2\ngoogle-cloud-storage           1.30.0\ngoogle-cloud-tasks             1.5.0\ngoogle-cloud-translate         3.1.0\ngoogle-cloud-videointelligence 2.1.0\ngoogle-cloud-vision            2.3.1\ngoogle-crc32c                  1.1.2\ngoogle-pasta                   0.2.0\ngoogle-resumable-media         1.2.0\ngoogleapis-common-protos       1.53.0\ngplearn                        0.4.1\ngpustat                        0.6.0\ngpxpy                          1.4.2\ngraphviz                       0.8.4\ngreenlet                       1.0.0\ngrpc-google-iam-v1             0.12.3\ngrpcio                         1.32.0\ngrpcio-gcp                     0.2.2\ngym                            0.18.0\nh2o                            3.32.1.1\nh5py                           2.10.0\nhaversine                      2.3.0\nHeapDict                       1.0.1\nhep-ml                         0.6.2\nhijri-converter                2.1.1\nhiredis                        2.0.0\nhmmlearn                       0.2.5\nholidays                       0.11.1\nholoviews                      1.14.3\nhpsklearn                      0.1.0\nhtml5lib                       1.1\nhtmlmin                        0.1.12\nhttplib2                       0.19.0\nhttplib2shim                   0.0.3\nhumanize                       3.4.1\nhunspell                       0.5.5\nhusl                           4.0.3\nhyperopt                       0.2.5\nhypertools                     0.6.3\nhypothesis                     6.10.0\nibis-framework                 1.4.0\nidna                           2.10\nimagecodecs                    2021.3.31\nImageHash                      4.2.0\nimageio                        2.9.0\nimbalanced-learn               0.8.0\nimgaug                         0.4.0\nimplicit                       0.4.4\nimportlib-metadata             3.4.0\niniconfig                      1.1.1\nipykernel                      5.5.0\nipympl                         0.7.0\nipython                        7.22.0\nipython-genutils               0.2.0\nipython-sql                    0.3.9\nipywidgets                     7.6.3\niso3166                        1.0.1\nisoweek                        1.3.3\nitsdangerous                   1.1.0\nJanome                         0.4.1\njax                            0.2.12\njaxlib                         0.1.64+cuda110\njedi                           0.18.0\njieba                          0.42.1\nJinja2                         2.11.3\njinja2-time                    0.2.0\njmespath                       0.10.0\njoblib                         1.0.1\njson5                          0.9.5\njsonnet                        0.17.0\njsonschema                     3.2.0\njupyter-client                 6.1.12\njupyter-console                6.4.0\njupyter-core                   4.7.1\njupyter-http-over-ws           0.0.8\njupyterlab                     1.2.16\njupyterlab-git                 0.11.0\njupyterlab-pygments            0.1.2\njupyterlab-server              1.2.0\njupyterlab-widgets             1.0.0\nkaggle                         1.5.12\nkaggle-environments            1.7.11\nKeras                          2.4.3\nKeras-Preprocessing            1.1.2\nkeras-tuner                    1.0.2\nkiwisolver                     1.3.1\nkmapper                        2.0.0\nkmodes                         0.11.0\nknnimpute                      0.1.0\nkorean-lunar-calendar          0.2.1\nkornia                         0.5.0\nkubernetes                     12.0.1\nlangid                         1.1.6\nlearntools                     0.3.4\nleven                          1.0.4\nlibcst                         0.3.18\nlibrosa                        0.8.0\nlightfm                        1.16\nlightgbm                       3.2.0\nlime                           0.2.0.1\nline-profiler                  3.1.0\nllvmlite                       0.36.0\nlmdb                           1.2.0\nlml                            0.1.0\nlocket                         0.2.1\nLunarCalendar                  0.0.9\nlxml                           4.6.3\nMako                           1.1.4\nmapclassify                    2.4.2\nmarisa-trie                    0.7.5\nMarkdown                       3.3.4\nmarkovify                      0.9.0\nMarkupSafe                     1.1.1\nmatplotlib                     3.4.1\nmatplotlib-venn                0.11.6\nmatrixprofile                  1.1.10\nmccabe                         0.6.1\nmemory-profiler                0.58.0\nmercantile                     1.1.6\nmissingno                      0.4.2\nmistune                        0.8.4\nmizani                         0.7.3\nml-metrics                     0.1.4\nmlcrate                        0.2.0\nmlens                          0.2.3\nmlxtend                        0.18.0\nmmh3                           3.0.0\nmne                            0.22.1\nmnist                          0.2.2\nmock                           4.0.3\nmore-itertools                 8.7.0\nmpld3                          0.5.2\nmpmath                         1.2.1\nmsgpack                        1.0.2\nmsgpack-numpy                  0.4.7.1\nmultidict                      5.1.0\nmultipledispatch               0.6.0\nmultiprocess                   0.70.11.1\nmunch                          2.5.0\nmurmurhash                     1.0.5\nmxnet-cu110                    1.8.0.post0\nmypy-extensions                0.4.3\nnb-conda                       2.2.1\nnb-conda-kernels               2.3.1\nnbclient                       0.5.3\nnbconvert                      6.0.7\nnbdime                         2.1.0\nnbformat                       5.1.2\nnest-asyncio                   1.4.3\nnetCDF4                        1.5.6\nnetworkx                       2.5\nnibabel                        3.2.1\nnilearn                        0.7.1\nnltk                           3.2.4\nnnabla                         1.19.0\nnnabla-ext-cuda110             1.19.0\nnose                           1.3.7\nnotebook                       6.3.0\nnotebook-executor              0.2\nnumba                          0.53.1\nnumexpr                        2.7.3\nnumpy                          1.19.5\nnvidia-ml-py3                  7.352.0\noauth2client                   4.1.3\noauthlib                       3.0.1\nodfpy                          1.4.1\nolefile                        0.46\nonnx                           1.8.1\nopencensus                     0.7.12\nopencensus-context             0.1.2\nopencv-python                  4.5.1.48\nopencv-python-headless         4.5.1.48\nopenslide-python               1.1.2\nopt-einsum                     3.3.0\noptuna                         2.7.0\norderedmultidict               1.0.1\nortools                        8.2.8710\nosmnx                          1.0.1\nosqp                           0.6.2.post0\noverrides                      3.1.0\npackaging                      20.9\npalettable                     3.3.0\npandas                         1.1.5\npandas-datareader              0.9.0\npandas-profiling               2.11.0\npandas-summary                 0.0.7\npandasql                       0.7.3\npandocfilters                  1.4.2\npanel                          0.11.3\npapermill                      2.3.3\nparam                          1.10.1\nparamiko                       2.7.2\nparso                          0.8.1\npartd                          1.2.0\npath                           15.1.2\npath.py                        12.5.0\npathos                         0.2.7\npathspec                       0.8.1\npathtools                      0.1.2\npatsy                          0.5.1\npbr                            5.5.1\npdf2image                      1.14.0\nPDPbox                         0.2.1\npexpect                        4.8.0\nphik                           0.11.2\npickleshare                    0.7.5\nPillow                         7.2.0\npip                            21.0.1\nplac                           1.1.3\nplotly                         4.14.3\nplotly-express                 0.4.1\nplotnine                       0.8.0\npluggy                         0.13.1\npolyglot                       16.7.4\npooch                          1.3.0\nportalocker                    2.3.0\npox                            0.2.9\npoyo                           0.5.0\nppca                           0.0.4\nppft                           1.6.6.3\npreprocessing                  0.1.13\npreshed                        3.0.5\nprettytable                    2.1.0\nprometheus-client              0.9.0\npromise                        2.3\nprompt-toolkit                 3.0.18\npronouncing                    0.2.0\nproto-plus                     1.18.1\nprotobuf                       3.15.8\npsutil                         5.8.0\nptyprocess                     0.7.0\npudb                           2020.1\npy                             1.10.0\npy-lz4framed                   0.14.0\npy-spy                         0.3.5\npy-stringmatching              0.4.2\npy-stringsimjoin               0.3.2\npyaml                          20.4.0\nPyArabic                       0.6.10\npyarrow                        1.0.1\npyasn1                         0.4.8\npyasn1-modules                 0.2.7\nPyAstronomy                    0.16.0\npybind11                       2.6.2\npycodestyle                    2.7.0\npycosat                        0.6.3\npycountry                      20.7.3\npycparser                      2.20\npycrypto                       2.6.1\npyct                           0.4.8\npycuda                         2021.1\npydash                         5.0.0\npydegensac                     0.1.2\npydicom                        2.1.2\npydot                          1.4.2\npydub                          0.25.1\npyemd                          0.5.1\npyexcel-io                     0.6.4\npyexcel-ods                    0.6.0\npyfasttext                     0.4.6\npyflakes                       2.3.1\npyglet                         1.5.0\nPygments                       2.8.1\nPyJWT                          2.0.1\npykalman                       0.9.5\npyLDAvis                       3.3.1\npymc3                          3.11.2\nPyMeeus                        0.5.11\npymongo                        3.11.3\nPympler                        0.9\nPyNaCl                         1.4.0\npynndescent                    0.5.2\npynvml                         8.0.4\npynvrtc                        9.2\npyocr                          0.8\npyOpenSSL                      20.0.1\npyparsing                      2.4.7\npyPdf                          1.13\npyperclip                      1.8.2\nPyPrind                        2.11.3\npyproj                         2.6.1.post1\nPyQt5                          5.12.3\nPyQt5-sip                      4.19.18\nPyQtChart                      5.12\nPyQtWebEngine                  5.12.1\npyrsistent                     0.17.3\npysal                          2.1.0\npyshp                          2.1.3\nPySocks                        1.7.1\npystan                         2.19.1.1\npytesseract                    0.3.7\npytest                         6.2.3\npytext-nlp                     0.1.2\npython-bidi                    0.4.2\npython-dateutil                2.8.1\npython-editor                  1.0.4\npython-igraph                  0.9.1\npython-Levenshtein             0.12.2\npython-louvain                 0.15\npython-slugify                 4.0.1\npytools                        2021.2.3\npytorch-ignite                 0.4.4\npytorch-lightning              1.2.8\npytz                           2021.1\nPyUpSet                        0.1.1.post7\npyviz-comms                    2.0.1\nPyWavelets                     1.1.1\nPyYAML                         5.3.1\npyzmq                          22.0.3\nqdldl                          0.1.5.post0\nqgrid                          1.3.1\nqtconsole                      5.0.3\nQtPy                           1.9.0\nrandomgen                      1.16.6\nrasterio                       1.2.2\nray                            1.2.0\nredis                          3.5.3\nregex                          2021.3.17\nrequests                       2.25.1\nrequests-oauthlib              1.3.0\nresampy                        0.2.2\nretrying                       1.3.3\nrgf-python                     3.9.0\nrmm                            0.16.0\nrsa                            4.7.2\nRtree                          0.9.7\nruamel-yaml-conda              0.15.80\ns2sphere                       0.2.5\ns3fs                           0.6.0\ns3transfer                     0.3.7\nsacremoses                     0.0.45\nscattertext                    0.1.2\nscikit-image                   0.18.1\nscikit-learn                   0.24.1\nscikit-multilearn              0.2.0\nscikit-optimize                0.8.1\nscikit-plot                    0.3.7\nscikit-surprise                1.1.1\nscipy                          1.5.4\nscs                            2.1.3\nseaborn                        0.11.1\nsemver                         2.13.0\nSend2Trash                     1.5.0\nsentencepiece                  0.1.95\nsentry-sdk                     1.0.0\nsetuptools                     49.6.0.post20210108\nsetuptools-git                 1.2\nshap                           0.39.0\nShapely                        1.7.1\nshortuuid                      1.0.1\nSimpleITK                      2.0.2\nsimplejson                     3.17.2\nsix                            1.15.0\nsklearn                        0.0\nsklearn-contrib-py-earth       0.1.0+1.gdde5f89\nsklearn-pandas                 2.1.0\nslicer                         0.0.7\nsmart-open                     5.0.0\nsmhasher                       0.150.1\nsmmap                          3.0.5\nsnuggs                         1.4.7\nsortedcontainers               2.3.0\nSoundFile                      0.10.3.post1\nspacy                          2.3.5\nspectral                       0.22.2\nsphinx-rtd-theme               0.2.4\nSQLAlchemy                     1.4.3\nsqlparse                       0.4.1\nsquarify                       0.4.3\nsrsly                          1.0.5\nstatsmodels                    0.12.2\nstemming                       1.0.1\nstevedore                      3.3.0\nstop-words                     2018.7.23\nstopit                         1.1.2\nstumpy                         1.8.0\nsubprocess32                   3.5.4\nsympy                          1.8\ntables                         3.6.1\ntabulate                       0.8.9\ntangled-up-in-unicode          0.0.7\ntblib                          1.7.0\ntenacity                       7.0.0\ntensorboard                    2.4.1\ntensorboard-plugin-wit         1.8.0\ntensorboardX                   2.2\ntensorflow                     2.4.1\ntensorflow-addons              0.12.1\ntensorflow-cloud               0.1.13\ntensorflow-datasets            3.0.0\ntensorflow-estimator           2.4.0\ntensorflow-gcs-config          2.1.7\ntensorflow-hub                 0.12.0\ntensorflow-metadata            0.29.0\ntensorflow-probability         0.12.2\ntensorpack                     0.11\ntermcolor                      1.1.0\nterminado                      0.9.3\nterminaltables                 3.1.0\ntestpath                       0.4.4\ntext-unidecode                 1.3\ntextblob                       0.15.3\ntexttable                      1.6.3\ntextwrap3                      0.9.2\nTheano                         1.0.5\nTheano-PyMC                    1.1.2\nthinc                          7.4.5\nthreadpoolctl                  2.1.0\ntifffile                       2021.4.8\ntokenizers                     0.10.2\ntoml                           0.10.2\ntoolz                          0.11.1\ntorch                          1.7.0\ntorchaudio                     0.7.0a0+ac17b64\ntorchmetrics                   0.2.0\ntorchtext                      0.8.0a0+cd6902d\ntorchvision                    0.8.1\ntornado                        6.1\nTPOT                           0.11.7\ntqdm                           4.59.0\ntraitlets                      5.0.5\ntraittypes                     0.2.1\ntransformers                   4.5.1\ntreelite                       0.93\ntreelite-runtime               0.93\ntrueskill                      0.4.5\ntsfresh                        0.18.0\ntyped-ast                      1.4.2\ntypeguard                      2.12.0\ntyping-extensions              3.7.4.3\ntyping-inspect                 0.6.0\ntzlocal                        2.1\nucx-py                         0.16.0\numap-learn                     0.5.1\nUnidecode                      1.2.0\nupdate-checker                 0.18.0\nuritemplate                    3.0.1\nurllib3                        1.26.4\nurwid                          2.1.2\nvecstack                       0.4.0\nvisions                        0.6.0\nvowpalwabbit                   8.10.1\nvtk                            9.0.1\nWand                           0.6.6\nwandb                          0.10.26\nwasabi                         0.8.2\nwavio                          0.0.4\nwcwidth                        0.2.5\nwebencodings                   0.5.1\nwebsocket-client               0.57.0\nWerkzeug                       1.0.1\nwfdb                           3.3.0\nwheel                          0.36.2\nwhichcraft                     0.6.1\nwidgetsnbextension             3.5.1\nWordbatch                      1.4.6\nwordcloud                      1.8.1\nwordsegment                    1.3.1\nwrapt                          1.12.1\nxarray                         0.17.0\nxgboost                        1.4.0\nxvfbwrapper                    0.2.9\nyacs                           0.1.8\nyarl                           1.6.3\nyellowbrick                    1.3.post1\nzict                           2.0.0\nzipp                           3.4.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu110.html\n!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu110.html\n!pip install -q torch-geometric\n!pip install ogb","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting ogb\n  Downloading ogb-1.3.1-py3-none-any.whl (67 kB)\n\u001b[K     |████████████████████████████████| 67 kB 652 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.1.5)\nRequirement already satisfied: tqdm>=4.29.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (4.59.0)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (0.24.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.7.0)\nCollecting outdated>=0.2.0\n  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\nRequirement already satisfied: urllib3>=1.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.26.4)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.15.0)\nRequirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.19.5)\nCollecting littleutils\n  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from outdated>=0.2.0->ogb) (2.25.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2021.1)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (1.0.1)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (1.5.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (2.1.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->ogb) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->ogb) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->ogb) (0.6)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->outdated>=0.2.0->ogb) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->outdated>=0.2.0->ogb) (4.0.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->outdated>=0.2.0->ogb) (2020.12.5)\nBuilding wheels for collected packages: littleutils\n  Building wheel for littleutils (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=7b15234a3b74150d5650b420fee088230a983ff3db87ec9e5f57e49e5f975167\n  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\nSuccessfully built littleutils\nInstalling collected packages: littleutils, outdated, ogb\nSuccessfully installed littleutils-0.2.2 ogb-1.3.1 outdated-0.2.1\n","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu102.html\n# !pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu102.html\n# !pip install -q torch-geometric\n# !pip install ogb","metadata":{"id":"By2oyBw7Lrh5","trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# 1 PyTorch Geometric (Datasets and Data)\n","metadata":{"id":"Nwwq0nSdmsOL"}},{"cell_type":"markdown","source":"PyTorch Geometric generally has two classes for storing or transforming the graphs into tensor format. One is the `torch_geometric.datasets`, which contains a variety of common graph datasets. Another one is `torch_geometric.data` that provides the data handling of graphs in PyTorch tensors.\n\nIn this section, we will learn how to use the `torch_geometric.datasets` and `torch_geometric.data`.","metadata":{"id":"Sf7vUmdNKCjA"}},{"cell_type":"markdown","source":"## PyG Datasets\n\nThe `torch_geometric.datasets` has many common graph datasets. Here we will explore the usage by using one example dataset.","metadata":{"id":"ic-o1P3r6hr2"}},{"cell_type":"code","source":"from torch_geometric.datasets import TUDataset\n\nroot = './enzymes'\nname = 'ENZYMES'\n\n# The ENZYMES dataset\npyg_dataset= TUDataset('./enzymes', 'ENZYMES')\n\n# You can find that there are 600 graphs in this dataset\nprint(pyg_dataset)","metadata":{"id":"zT5qca3x6XpG","trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading https://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip\nExtracting enzymes/ENZYMES/ENZYMES.zip\nProcessing...\nDone!\nENZYMES(600)\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls","metadata":{"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"__notebook_source__.ipynb  enzymes\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Question 1: What is the number of classes and number of features in the ENZYMES dataset? (5 points)","metadata":{"id":"NLm5vVYMAP2x"}},{"cell_type":"code","source":"def get_num_classes(pyg_dataset):\n    \n    # TODO: Implement this function that takes a PyG dataset object\n    # and return the number of classes for that dataset.\n\n    num_classes = 0\n\n    ############# Your code here ############\n    ## (~1 line of code)\n    ## Note\n    ## 1. Colab autocomplete functionality might be useful.\n    num_classes = pyg_dataset.num_classes\n    \n\n\n    #########################################\n\n    return num_classes\n\ndef get_num_features(pyg_dataset):\n  # TODO: Implement this function that takes a PyG dataset object\n  # and return the number of features for that dataset.\n\n    num_features = 0\n\n    ############# Your code here ############\n    ## (~1 line of code)\n    ## Note\n    ## 1. Colab autocomplete functionality might be useful.\n    num_features = pyg_dataset.num_features\n\n    #########################################\n\n    return num_features\n\n# You may find that some information need to be stored in the dataset level,\n# specifically if there are multiple graphs in the dataset\n\nnum_classes = get_num_classes(pyg_dataset)\nnum_features = get_num_features(pyg_dataset)\nprint(\"{} dataset has {} classes\".format(name, num_classes))\nprint(\"{} dataset has {} features\".format(name, num_features))","metadata":{"id":"8iF_Kyqr_JbY","trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"ENZYMES dataset has 6 classes\nENZYMES dataset has 3 features\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> The ENZYMES dataset has 600 graphs. The above functions allows us to look at the ENZYMES overall properties","metadata":{}},{"cell_type":"markdown","source":"## PyG Data\n\nEach PyG dataset usually stores a list of `torch_geometric.data.Data` objects. Each `torch_geometric.data.Data` object usually represents a graph. You can easily get the `Data` object by indexing on the dataset.\n\nFor more information such as what will be stored in `Data` object, please refer to the [documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data).","metadata":{"id":"rwKbzhHUAckZ"}},{"cell_type":"markdown","source":"## Question 2: What is the label of the graph (index 100 in the ENZYMES dataset)? (5 points)","metadata":{"id":"7sCV3xJWCddX"}},{"cell_type":"code","source":"def get_graph_class(pyg_dataset, idx):\n    # TODO: Implement this function that takes a PyG dataset object,\n    # the index of the graph in dataset, and returns the class/label \n    # of the graph (in integer).\n    \n    # here the idx refers to different graphs in the pyg_dataset\n\n    label = -1\n\n    ############# Your code here ############\n    ## (~1 line of code)\n    label = pyg_dataset[idx].y\n\n\n    #########################################\n\n    return label\n\n# Here pyg_dataset is a dataset for graph classification\ngraph_0 = pyg_dataset[0]\nprint(graph_0)\nidx = 100\nlabel = get_graph_class(pyg_dataset, idx)\nprint('Graph with index {} has label {}'.format(idx, label))","metadata":{"id":"LIis9oTZAfs3","trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Data(edge_index=[2, 168], x=[37, 3], y=[1])\nGraph with index 100 has label tensor([4])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Question 3: What is the number of edges for the graph (index 200 in the ENZYMES dataset)? (5 points)","metadata":{"id":"fKhcVeAhCwoY"}},{"cell_type":"code","source":"def get_graph_num_edges(pyg_dataset, idx):\n    # TODO: Implement this function that takes a PyG dataset object,\n    # the index of the graph in dataset, and returns the number of \n    # edges in the graph (in integer). You should not count an edge \n    # twice if the graph is undirected. For example, in an undirected \n    # graph G, if two nodes v and u are connected by an edge, this edge\n    # should only be counted once.\n    \n    # idx refers to a particular graph\n\n\n    num_edges = 0\n\n    ############# Your code here ############\n    ## Note:\n    ## 1. You can't return the data.num_edges directly\n    ## 2. We assume the graph is undirected\n    ## (~4 lines of code)\n    \n    \n    # lets iterate through the edge_index, (2,# of edge pairs from u to v)\n    num_edge_pairs = pyg_dataset[idx].edge_index.shape[1]\n    edge_pair_tensor = pyg_dataset[idx].edge_index\n    \n    # lets keep track of edges \n    edge_list = []\n    \n    for i in range(num_edge_pairs):\n        u = edge_pair_tensor[0,i]\n        v = edge_pair_tensor[1,i]\n        # check if the pair exist in edge_list,\n        # also considering undirected, if so, we skip\n        if ((u,v) in edge_list) or ((v,u) in edge_list):\n            continue\n        else:\n            edge_list.append((u,v))\n        \n    \n\n    #########################################\n    \n    num_edges = len(edge_list)\n    return num_edges\n\nidx = 200\nnum_edges = get_graph_num_edges(pyg_dataset, idx)\nprint('Graph with index {} has {} edges'.format(idx, num_edges))","metadata":{"id":"f5m2DOfhBtWv","trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Graph with index 200 has 53 edges\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2 Open Graph Benchmark (OGB)\n\nThe Open Graph Benchmark (OGB) is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. Its datasets are automatically downloaded, processed, and split using the OGB Data Loader. The model performance can also be evaluated by using the OGB Evaluator in a unified manner.","metadata":{"id":"AXa7yIG4E0Fp"}},{"cell_type":"markdown","source":"## Dataset and Data\n\nOGB also supports the PyG dataset and data. Here we take a look on the `ogbn-arxiv` dataset.","metadata":{"id":"HnazPGGAJAZN"}},{"cell_type":"code","source":"import torch_geometric.transforms as T\nfrom ogb.nodeproppred import PygNodePropPredDataset\n\ndataset_name = 'ogbn-arxiv'\n# Load the dataset and transform it to sparse tensor\ndataset = PygNodePropPredDataset(name=dataset_name,\n                                 transform=T.ToSparseTensor())\nprint('The {} dataset has {} graph'.format(dataset_name, len(dataset)))\n\n# Extract the graph\ndata = dataset[0] # we have 1 dataset anyway\nprint(data)","metadata":{"id":"Gpc6bTm3GF02","trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n","output_type":"stream"},{"name":"stderr","text":"Downloaded 0.08 GB: 100%|██████████| 81/81 [01:09<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting dataset/arxiv.zip\nProcessing...\nLoading necessary files...\nThis might take a while.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00, 9058.97it/s]\n100%|██████████| 1/1 [00:00<00:00, 5607.36it/s]","output_type":"stream"},{"name":"stdout","text":"Processing graphs...\nConverting graphs into PyG objects...\nSaving...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Done!\nThe ogbn-arxiv dataset has 1 graph\nData(adj_t=[169343, 169343, nnz=1166243], node_year=[169343, 1], x=[169343, 128], y=[169343, 1])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Question 4: What is the number of features in the ogbn-arxiv graph? (5 points)","metadata":{"id":"Cw0xZJKZI-n3"}},{"cell_type":"code","source":"def graph_num_features(data):\n    # TODO: Implement this function that takes a PyG data object,\n    # and returns the number of features in the graph (in integer).\n\n    num_features = 0\n\n    ############# Your code here ############\n    ## (~1 line of code)\n    num_features = data.num_features # (or alternative, we can just take .x.shape[1])\n\n    #########################################\n\n    return num_features\n\nnum_features = graph_num_features(data)\nprint('The graph has {} features'.format(num_features))","metadata":{"id":"ZP844_nT2ZJl","trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"The graph has 128 features\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 3 GNN: Node Property Prediction\n\nIn this section we will build our first graph neural network by using PyTorch Geometric and apply it on node property prediction (node classification).\n\nWe will build the graph neural network by using GCN operator ([Kipf et al. (2017)](https://arxiv.org/pdf/1609.02907.pdf)).\n\nYou should use the PyG built-in `GCNConv` layer directly. ","metadata":{"id":"9DP_yEQZ0NVW"}},{"cell_type":"markdown","source":"## Setup","metadata":{"id":"O4CcOUEoInjD"}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nprint(torch.__version__)\n\n# The PyG built-in GCNConv\nfrom torch_geometric.nn import GCNConv\n\nimport torch_geometric.transforms as T\nfrom ogb.nodeproppred import PygNodePropPredDataset, Evaluator","metadata":{"id":"-DCtgcHpGIpd","trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"1.7.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load and Preprocess the Dataset","metadata":{"id":"0IK9z0wQIwzQ"}},{"cell_type":"code","source":"dataset_name = 'ogbn-arxiv'\ndataset = PygNodePropPredDataset(name=dataset_name,\n                                 transform=T.ToSparseTensor())\ndata = dataset[0]\n\n# Make the adjacency matrix to symmetric\n# https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/transforms/to_undirected.html\ndata.adj_t = data.adj_t.to_symmetric() # this makes it undirected \n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# If you use GPU, the device should be cuda\nprint('Device: {}'.format(device))\n\ndata = data.to(device)\n\"\"\"\n{'train': tensor([     0,      1,      2,  ..., 169145, 169148, 169251]),\n 'valid': tensor([   349,    357,    366,  ..., 169185, 169261, 169296]),\n 'test': tensor([   346,    398,    451,  ..., 169340, 169341, 169342])}\n\"\"\"\nsplit_idx = dataset.get_idx_split() # dict that contains train node index, valid node index and test node index \ntrain_idx = split_idx['train'].to(device)","metadata":{"id":"0ibJ0ieoIwQM","trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## GCN Model\n\nNow we will implement our GCN model!\n\nPlease follow the figure below to implement your `forward` function.\n\n\n![test](https://drive.google.com/uc?id=128AuYAXNXGg7PIhJJ7e420DoPWKb-RtL)","metadata":{"id":"OgUA815bNJ8w"}},{"cell_type":"code","source":"class GCN(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n                 dropout, return_embeds=False):\n        # TODO: Implement this function that initializes self.convs, \n        # self.bns, and self.softmax.\n\n        super(GCN, self).__init__()\n        self.num_layers = num_layers\n\n        # A list of GCNConv layers\n        # https://discuss.pytorch.org/t/when-should-i-use-nn-modulelist-and-when-should-i-use-nn-sequential/5463/4\n        self.convs = torch.nn.ModuleList([GCNConv(in_channels=input_dim,out_channels=hidden_dim)])\n        # extend if needed with list comprehension\n        # append if needed\n        self.convs.extend([GCNConv(in_channels=hidden_dim,out_channels=hidden_dim) for _ in range(1,num_layers-1)])\n        self.convs.append(GCNConv(in_channels=hidden_dim, out_channels=output_dim)) # last GCN\n\n        # A list of 1D batch normalization layers\n        self.bns = torch.nn.ModuleList([torch.nn.BatchNorm1d(num_features=hidden_dim) for _ in range(1,num_layers)])\n\n        # The log softmax layer\n        \n        # because later down in the assignment, nll_loss is use, so we need to convert the output to log-probabilities\n        # https://stackoverflow.com/questions/49036993/pytorch-softmax-what-dimension-to-use\n        # each row's output_dim logic should sum to 1\n        self.softmax = torch.nn.LogSoftmax(dim=1)\n\n        ############# Your code here ############\n        ## Note:\n        ## 1. You should use torch.nn.ModuleList for self.convs and self.bns\n        ## 2. self.convs has num_layers GCNConv layers\n        ## 3. self.bns has num_layers - 1 BatchNorm1d layers\n        ## 4. You should use torch.nn.LogSoftmax for self.softmax\n        ## 5. The parameters you can set for GCNConv include 'in_channels' and \n        ## 'out_channels'. More information please refer to the documentation:\n        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv\n        ## 6. The only parameter you need to set for BatchNorm1d is 'num_features'\n        ## More information please refer to the documentation: \n        ## https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n        ## (~10 lines of code)\n\n        #########################################\n\n        # Probability of an element to be zeroed\n        self.dropout = dropout\n\n        # Skip classification layer and return node embeddings\n        self.return_embeds = return_embeds\n\n    def reset_parameters(self):\n        for conv in self.convs:\n            conv.reset_parameters()\n        for bn in self.bns:\n            bn.reset_parameters()\n\n    def forward(self, x, adj_t):\n        \"\"\"\n        Parameters\n        ----------\n        x: feature tensor\n        adj_t: SparseTensor that is like edge_index, pointing u to v\n        \"\"\"\n        # TODO: Implement this function that takes the feature tensor x,\n        # edge_index tensor adj_t and returns the output tensor as\n        # shown in the figure.\n\n        out = None\n\n        ############# Your code here ############\n        ## Note:\n        ## 1. Construct the network as showing in the figure\n        ## 2. torch.nn.functional.relu and torch.nn.functional.dropout are useful\n        ## More information please refer to the documentation:\n        ## https://pytorch.org/docs/stable/nn.functional.html\n        ## 3. Don't forget to set F.dropout training to self.training\n        ## 4. If return_embeds is True, then skip the last softmax layer\n        ## (~7 lines of code)\n        \n        for i in range(self.num_layers):\n            if i == 0:\n                # first layer\n                h = self.convs[i](x,adj_t)\n                h = self.bns[i](h)\n                h = torch.nn.ReLU()(h)\n                h = torch.nn.Dropout(self.dropout)(h)\n            elif i == self.num_layers-1:\n                # last layer\n                h = self.convs[i](h,adj_t)\n                out = self.softmax(h) # converts logit to log prob for n_ll later\n            else:\n                h = self.convs[i](h,adj_t)\n                h = self.bns[i](h)\n                h = torch.nn.ReLU()(h)\n                h = torch.nn.Dropout(self.dropout)(h)\n\n        #########################################\n\n        return out","metadata":{"id":"IgspXTYpNJLA","trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# test architecture\ntemp_model = GCN(12,5,4,3,0.2)\nprint(temp_model)\ndel temp_model","metadata":{"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"GCN(\n  (convs): ModuleList(\n    (0): GCNConv(12, 5)\n    (1): GCNConv(5, 5)\n    (2): GCNConv(5, 4)\n  )\n  (bns): ModuleList(\n    (0): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (1): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (softmax): LogSoftmax(dim=1)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"def train(model, data, train_idx, optimizer, loss_fn):\n    \"\"\"\n    Parameter\n    ---------\n    model: model class\n    data: torch_geometric.data.data.Data\n    train_idx: index of train nodes\n    optimzer:\n    loss_fn: loss criterion\n    \"\"\"\n    # TODO: Implement this function that trains the model by \n    # using the given optimizer and loss_fn.\n    \n    \n    # toggles layers dropout,bn for training\n    # https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch\n    model.train()\n    loss = 0\n\n    ############# Your code here ############\n    ## Note:\n    ## 1. Zero grad the optimizer\n    ## 2. Feed the data into the model\n    ## 3. Slicing the model output and label by train_idx\n    ## 4. Feed the sliced output and label to loss_fn\n    ## (~4 lines of code)\n\n    #########################################\n    optimizer.zero_grad()\n    \n    y_pred = model(data.x, data.adj_t) # model expects x and edge_index(adj_t)\n    y_train = y_pred[train_idx]\n    # https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n    loss = loss_fn(y_train,data.y[train_idx].squeeze())\n    \n\n    loss.backward()\n    optimizer.step()\n\n    return loss.item()","metadata":{"id":"FF1hnHUhO81e","trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# Test function here\n@torch.no_grad()\ndef test(model, data, split_idx, evaluator):\n    # TODO: Implement this function that tests the model by \n    # using the given split_idx and evaluator.\n    model.eval()\n\n    # The output of model on all data\n    out = None\n\n    ############# Your code here ############\n    ## (~1 line of code)\n    ## Note:\n    ## 1. No index slicing here\n    out = model(data.x,data.adj_t)\n\n    #########################################\n\n    y_pred = out.argmax(dim=-1, keepdim=True)\n\n    train_acc = evaluator.eval({\n        'y_true': data.y[split_idx['train']],\n        'y_pred': y_pred[split_idx['train']],\n    })['acc']\n    valid_acc = evaluator.eval({\n        'y_true': data.y[split_idx['valid']],\n        'y_pred': y_pred[split_idx['valid']],\n    })['acc']\n    test_acc = evaluator.eval({\n        'y_true': data.y[split_idx['test']],\n        'y_pred': y_pred[split_idx['test']],\n    })['acc']\n\n    return train_acc, valid_acc, test_acc","metadata":{"id":"aJdlrJQhPBsK","trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# Please do not change the args\nargs = {\n    'device': device,\n    'num_layers': 3,\n    'hidden_dim': 256,\n    'dropout': 0.5,\n    'lr': 0.01,\n    'epochs': 100,\n}\nargs","metadata":{"id":"o7F46xkuLiOL","trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"{'device': 'cuda',\n 'num_layers': 3,\n 'hidden_dim': 256,\n 'dropout': 0.5,\n 'lr': 0.01,\n 'epochs': 100}"},"metadata":{}}]},{"cell_type":"code","source":"model = GCN(data.num_features, args['hidden_dim'],\n            dataset.num_classes, args['num_layers'],\n            args['dropout']).to(device)\nevaluator = Evaluator(name='ogbn-arxiv')","metadata":{"id":"dT8RyM2cPGxM","trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"import copy\n\n# reset the parameters to initial random value\nmodel.reset_parameters()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\nloss_fn = F.nll_loss\n\nbest_model = None\nbest_valid_acc = 0\n\nfor epoch in range(1, 1 + args[\"epochs\"]):    \n    loss = train(model, data, train_idx, optimizer, loss_fn)\n    result = test(model, data, split_idx, evaluator)\n    train_acc, valid_acc, test_acc = result\n    if valid_acc > best_valid_acc:\n        best_valid_acc = valid_acc\n        best_model = copy.deepcopy(model)\n    print(f'Epoch: {epoch:02d}, '\n        f'Loss: {loss:.4f}, '\n        f'Train: {100 * train_acc:.2f}%, '\n        f'Valid: {100 * valid_acc:.2f}% '\n        f'Test: {100 * test_acc:.2f}%')","metadata":{"id":"qd5O5cnPPdVF","scrolled":true,"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"Epoch: 01, Loss: 3.9560, Train: 30.71%, Valid: 35.04% Test: 35.74%\nEpoch: 02, Loss: 2.4273, Train: 22.25%, Valid: 18.40% Test: 16.33%\nEpoch: 03, Loss: 1.9620, Train: 21.10%, Valid: 10.51% Test: 8.55%\nEpoch: 04, Loss: 1.8410, Train: 27.78%, Valid: 18.52% Test: 18.27%\nEpoch: 05, Loss: 1.6924, Train: 33.47%, Valid: 20.69% Test: 18.52%\nEpoch: 06, Loss: 1.6241, Train: 36.55%, Valid: 22.51% Test: 18.94%\nEpoch: 07, Loss: 1.5561, Train: 38.61%, Valid: 26.67% Test: 25.02%\nEpoch: 08, Loss: 1.4846, Train: 41.39%, Valid: 34.23% Test: 35.38%\nEpoch: 09, Loss: 1.4258, Train: 42.75%, Valid: 36.67% Test: 39.35%\nEpoch: 10, Loss: 1.3901, Train: 43.86%, Valid: 38.67% Test: 41.64%\nEpoch: 11, Loss: 1.3559, Train: 44.04%, Valid: 38.09% Test: 40.92%\nEpoch: 12, Loss: 1.3321, Train: 44.27%, Valid: 36.43% Test: 37.55%\nEpoch: 13, Loss: 1.3082, Train: 45.98%, Valid: 39.82% Test: 41.57%\nEpoch: 14, Loss: 1.2914, Train: 48.73%, Valid: 43.87% Test: 45.93%\nEpoch: 15, Loss: 1.2662, Train: 51.78%, Valid: 48.45% Test: 50.74%\nEpoch: 16, Loss: 1.2492, Train: 53.76%, Valid: 51.71% Test: 53.78%\nEpoch: 17, Loss: 1.2382, Train: 55.62%, Valid: 55.01% Test: 55.75%\nEpoch: 18, Loss: 1.2211, Train: 57.16%, Valid: 56.33% Test: 55.90%\nEpoch: 19, Loss: 1.2089, Train: 57.62%, Valid: 56.52% Test: 55.60%\nEpoch: 20, Loss: 1.1913, Train: 58.83%, Valid: 57.84% Test: 56.87%\nEpoch: 21, Loss: 1.1811, Train: 59.01%, Valid: 58.30% Test: 57.71%\nEpoch: 22, Loss: 1.1764, Train: 58.54%, Valid: 57.56% Test: 57.57%\nEpoch: 23, Loss: 1.1631, Train: 58.72%, Valid: 57.83% Test: 57.88%\nEpoch: 24, Loss: 1.1493, Train: 58.63%, Valid: 57.45% Test: 58.05%\nEpoch: 25, Loss: 1.1428, Train: 59.01%, Valid: 57.55% Test: 58.33%\nEpoch: 26, Loss: 1.1329, Train: 60.13%, Valid: 58.82% Test: 59.85%\nEpoch: 27, Loss: 1.1305, Train: 60.94%, Valid: 59.90% Test: 61.36%\nEpoch: 28, Loss: 1.1233, Train: 61.77%, Valid: 61.30% Test: 63.13%\nEpoch: 29, Loss: 1.1135, Train: 61.80%, Valid: 61.46% Test: 63.87%\nEpoch: 30, Loss: 1.1102, Train: 62.42%, Valid: 61.83% Test: 63.75%\nEpoch: 31, Loss: 1.1032, Train: 63.55%, Valid: 62.74% Test: 64.00%\nEpoch: 32, Loss: 1.0966, Train: 64.22%, Valid: 63.56% Test: 65.07%\nEpoch: 33, Loss: 1.0878, Train: 65.18%, Valid: 64.97% Test: 65.75%\nEpoch: 34, Loss: 1.0833, Train: 65.60%, Valid: 65.70% Test: 66.20%\nEpoch: 35, Loss: 1.0769, Train: 66.22%, Valid: 66.64% Test: 66.50%\nEpoch: 36, Loss: 1.0737, Train: 66.57%, Valid: 66.62% Test: 66.79%\nEpoch: 37, Loss: 1.0670, Train: 66.87%, Valid: 66.90% Test: 66.69%\nEpoch: 38, Loss: 1.0637, Train: 67.28%, Valid: 66.85% Test: 66.47%\nEpoch: 39, Loss: 1.0595, Train: 67.34%, Valid: 67.23% Test: 66.73%\nEpoch: 40, Loss: 1.0552, Train: 67.51%, Valid: 67.30% Test: 66.79%\nEpoch: 41, Loss: 1.0495, Train: 67.47%, Valid: 67.50% Test: 66.67%\nEpoch: 42, Loss: 1.0468, Train: 67.93%, Valid: 67.83% Test: 66.31%\nEpoch: 43, Loss: 1.0401, Train: 68.32%, Valid: 67.90% Test: 66.37%\nEpoch: 44, Loss: 1.0349, Train: 68.58%, Valid: 68.11% Test: 66.86%\nEpoch: 45, Loss: 1.0314, Train: 68.78%, Valid: 68.43% Test: 67.40%\nEpoch: 46, Loss: 1.0323, Train: 68.83%, Valid: 68.27% Test: 66.85%\nEpoch: 47, Loss: 1.0267, Train: 68.87%, Valid: 68.11% Test: 66.52%\nEpoch: 48, Loss: 1.0249, Train: 69.04%, Valid: 68.15% Test: 66.68%\nEpoch: 49, Loss: 1.0229, Train: 69.04%, Valid: 68.37% Test: 67.33%\nEpoch: 50, Loss: 1.0167, Train: 69.16%, Valid: 68.65% Test: 67.79%\nEpoch: 51, Loss: 1.0162, Train: 69.29%, Valid: 68.83% Test: 67.54%\nEpoch: 52, Loss: 1.0114, Train: 69.55%, Valid: 68.59% Test: 67.37%\nEpoch: 53, Loss: 1.0062, Train: 69.54%, Valid: 68.53% Test: 66.99%\nEpoch: 54, Loss: 1.0090, Train: 69.64%, Valid: 68.70% Test: 67.36%\nEpoch: 55, Loss: 1.0052, Train: 69.52%, Valid: 69.02% Test: 67.81%\nEpoch: 56, Loss: 0.9981, Train: 69.38%, Valid: 68.94% Test: 68.48%\nEpoch: 57, Loss: 0.9989, Train: 69.67%, Valid: 69.42% Test: 68.59%\nEpoch: 58, Loss: 0.9962, Train: 69.93%, Valid: 69.62% Test: 68.65%\nEpoch: 59, Loss: 0.9903, Train: 70.15%, Valid: 69.60% Test: 68.80%\nEpoch: 60, Loss: 0.9879, Train: 70.10%, Valid: 69.59% Test: 68.58%\nEpoch: 61, Loss: 0.9894, Train: 70.27%, Valid: 69.71% Test: 68.72%\nEpoch: 62, Loss: 0.9828, Train: 70.29%, Valid: 69.53% Test: 68.43%\nEpoch: 63, Loss: 0.9838, Train: 70.28%, Valid: 69.77% Test: 69.19%\nEpoch: 64, Loss: 0.9799, Train: 70.11%, Valid: 69.62% Test: 69.03%\nEpoch: 65, Loss: 0.9783, Train: 70.33%, Valid: 69.36% Test: 68.23%\nEpoch: 66, Loss: 0.9782, Train: 70.50%, Valid: 69.52% Test: 68.21%\nEpoch: 67, Loss: 0.9769, Train: 70.47%, Valid: 69.49% Test: 68.03%\nEpoch: 68, Loss: 0.9724, Train: 70.51%, Valid: 69.64% Test: 68.42%\nEpoch: 69, Loss: 0.9722, Train: 70.60%, Valid: 69.54% Test: 68.49%\nEpoch: 70, Loss: 0.9687, Train: 70.70%, Valid: 69.81% Test: 68.40%\nEpoch: 71, Loss: 0.9653, Train: 70.69%, Valid: 69.49% Test: 68.15%\nEpoch: 72, Loss: 0.9648, Train: 70.67%, Valid: 69.74% Test: 68.55%\nEpoch: 73, Loss: 0.9622, Train: 70.68%, Valid: 70.03% Test: 68.95%\nEpoch: 74, Loss: 0.9646, Train: 70.67%, Valid: 70.05% Test: 69.49%\nEpoch: 75, Loss: 0.9597, Train: 70.61%, Valid: 69.76% Test: 69.05%\nEpoch: 76, Loss: 0.9577, Train: 70.68%, Valid: 69.92% Test: 68.62%\nEpoch: 77, Loss: 0.9557, Train: 71.06%, Valid: 70.16% Test: 68.90%\nEpoch: 78, Loss: 0.9524, Train: 71.14%, Valid: 70.13% Test: 69.03%\nEpoch: 79, Loss: 0.9513, Train: 71.19%, Valid: 70.32% Test: 69.72%\nEpoch: 80, Loss: 0.9477, Train: 71.05%, Valid: 70.41% Test: 69.70%\nEpoch: 81, Loss: 0.9478, Train: 71.06%, Valid: 70.23% Test: 69.53%\nEpoch: 82, Loss: 0.9455, Train: 71.03%, Valid: 69.94% Test: 69.12%\nEpoch: 83, Loss: 0.9468, Train: 70.92%, Valid: 69.73% Test: 68.33%\nEpoch: 84, Loss: 0.9438, Train: 71.20%, Valid: 69.71% Test: 68.70%\nEpoch: 85, Loss: 0.9415, Train: 71.29%, Valid: 70.20% Test: 69.45%\nEpoch: 86, Loss: 0.9386, Train: 71.25%, Valid: 70.41% Test: 69.83%\nEpoch: 87, Loss: 0.9398, Train: 71.37%, Valid: 70.30% Test: 69.95%\nEpoch: 88, Loss: 0.9364, Train: 71.40%, Valid: 70.21% Test: 69.85%\nEpoch: 89, Loss: 0.9326, Train: 71.53%, Valid: 69.85% Test: 68.82%\nEpoch: 90, Loss: 0.9313, Train: 71.31%, Valid: 69.46% Test: 67.48%\nEpoch: 91, Loss: 0.9307, Train: 71.31%, Valid: 69.71% Test: 68.30%\nEpoch: 92, Loss: 0.9300, Train: 71.48%, Valid: 70.46% Test: 69.66%\nEpoch: 93, Loss: 0.9280, Train: 71.47%, Valid: 70.34% Test: 69.70%\nEpoch: 94, Loss: 0.9258, Train: 71.70%, Valid: 70.27% Test: 69.04%\nEpoch: 95, Loss: 0.9288, Train: 71.65%, Valid: 70.29% Test: 69.05%\nEpoch: 96, Loss: 0.9225, Train: 71.63%, Valid: 70.08% Test: 68.91%\nEpoch: 97, Loss: 0.9220, Train: 71.40%, Valid: 69.87% Test: 68.85%\nEpoch: 98, Loss: 0.9215, Train: 71.56%, Valid: 70.12% Test: 68.93%\nEpoch: 99, Loss: 0.9199, Train: 71.70%, Valid: 70.28% Test: 68.47%\nEpoch: 100, Loss: 0.9165, Train: 71.73%, Valid: 70.37% Test: 69.77%\n","output_type":"stream"}]},{"cell_type":"code","source":"best_result = test(best_model, data, split_idx, evaluator)\ntrain_acc, valid_acc, test_acc = best_result\nprint(f'Best model: '\n      f'Train: {100 * train_acc:.2f}%, '\n      f'Valid: {100 * valid_acc:.2f}% '\n      f'Test: {100 * test_acc:.2f}%')","metadata":{"id":"EqcextqOL2FX","trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"Best model: Train: 71.52%, Valid: 70.24% Test: 69.64%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Question 5: What are your `best_model` validation and test accuracy? Please report them on Gradescope. For example, for an accuracy such as 50.01%, just report 50.01 and please don't include the percent sign. (20 points)","metadata":{"id":"duMEg-olLjbJ"}},{"cell_type":"markdown","source":"# 4 GNN: Graph Property Prediction\n\nIn this section we will create a graph neural network for graph property prediction (graph classification)\n","metadata":{"id":"R8pOD6y80TyI"}},{"cell_type":"markdown","source":"## Load and preprocess the dataset","metadata":{"id":"vRg5VOEdQTa4"}},{"cell_type":"code","source":"from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\nfrom torch_geometric.data import DataLoader\nfrom tqdm.notebook import tqdm\n\n# Load the dataset \ndataset = PygGraphPropPredDataset(name='ogbg-molhiv')\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Device: {}'.format(device))\n\nsplit_idx = dataset.get_idx_split()\n\n# Check task type\nprint('Task type: {}'.format(dataset.task_type))","metadata":{"id":"LXb-O5QUIgTH","trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"Downloading http://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/hiv.zip\n","output_type":"stream"},{"name":"stderr","text":"Downloaded 0.00 GB: 100%|██████████| 3/3 [00:01<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting dataset/hiv.zip\nProcessing...\nLoading necessary files...\nThis might take a while.\n","output_type":"stream"},{"name":"stderr","text":" 17%|█▋        | 6866/41127 [00:00<00:00, 68648.94it/s]","output_type":"stream"},{"name":"stdout","text":"Processing graphs...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 41127/41127 [00:00<00:00, 67963.38it/s]\n 28%|██▊       | 11391/41127 [00:00<00:00, 113906.58it/s]","output_type":"stream"},{"name":"stdout","text":"Converting graphs into PyG objects...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 41127/41127 [00:00<00:00, 66629.23it/s] \n","output_type":"stream"},{"name":"stdout","text":"Saving...\nDone!\nDevice: cuda\nTask type: binary classification\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the data sets into dataloader\n# We will train the graph classification task on a batch of 32 graphs\n# Shuffle the order of graphs for training set\ntrain_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=32, shuffle=True, num_workers=0)\nvalid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False, num_workers=0)\ntest_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=32, shuffle=False, num_workers=0)","metadata":{"id":"7cHHbgW1c5hi","trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# Please do not change the args\nargs = {\n    'device': device,\n    'num_layers': 5,\n    'hidden_dim': 256,\n    'dropout': 0.5,\n    'lr': 0.001,\n    'epochs': 30,\n}\nargs","metadata":{"id":"AYrSnOj0Y4DK","trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"{'device': 'cuda',\n 'num_layers': 5,\n 'hidden_dim': 256,\n 'dropout': 0.5,\n 'lr': 0.001,\n 'epochs': 30}"},"metadata":{}}]},{"cell_type":"markdown","source":"Extra: for further understand of the dataset and graph prediction problem","metadata":{}},{"cell_type":"code","source":"# this is the number of output dim. 1 here means we are doing a logloss k = 2 problem \n# (since later we will use binary cross entropy)\ndataset.num_tasks","metadata":{"trusted":true},"execution_count":122,"outputs":[{"execution_count":122,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"# there are 41127 unique graphs. Each sample is a graph with its own x and y \nlen(dataset)","metadata":{"trusted":true},"execution_count":123,"outputs":[{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"41127"},"metadata":{}}]},{"cell_type":"code","source":"# each graph sample has varying nodes and 9 features\ndataset[0].x.shape","metadata":{"trusted":true},"execution_count":126,"outputs":[{"execution_count":126,"output_type":"execute_result","data":{"text/plain":"torch.Size([19, 9])"},"metadata":{}}]},{"cell_type":"code","source":"dataset[50].x.shape","metadata":{"trusted":true},"execution_count":131,"outputs":[{"execution_count":131,"output_type":"execute_result","data":{"text/plain":"torch.Size([18, 9])"},"metadata":{}}]},{"cell_type":"code","source":"# since we are doing a graph binary classification, each graph sample has 1 label\ndataset[0].y.shape","metadata":{"trusted":true},"execution_count":132,"outputs":[{"execution_count":132,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 1])"},"metadata":{}}]},{"cell_type":"code","source":"# we can see train_loader automatically concats the graphs into a 2 dimensional tensor on its node index dimension (dim =0)\nfor i, batch in enumerate(train_loader):\n    print(f\"Batch {i} ---\")\n    print(batch.x.shape)\n    print(batch.y.shape)\n    \n    if i == 2:\n        break","metadata":{"trusted":true},"execution_count":135,"outputs":[{"name":"stdout","text":"Batch 0 ---\ntorch.Size([831, 9])\ntorch.Size([32, 1])\nBatch 1 ---\ntorch.Size([778, 9])\ntorch.Size([32, 1])\nBatch 2 ---\ntorch.Size([835, 9])\ntorch.Size([32, 1])\n","output_type":"stream"}]},{"cell_type":"code","source":"# we can see the above that each batch consist of 32 labels (because there are 32 graphs)\n# but the features are all concat. So how does our model knows to output 32 labels given x concat tensor? \n# Which x features belong to which graph???!\n# Ans- the global pooling graph layer\n# which requires the batch tensor (this tensor contains index of the 32 graph for the batch.x data)\n\n# the model's global mean graph pooling layer will pool the features to (32,)","metadata":{"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"code","source":"# here is the dataloader's batch's batch index which indicates which row of batch.x belongs to graph i where i = 0,1/...batch_size\nbatch.batch","metadata":{"trusted":true},"execution_count":144,"outputs":[{"execution_count":144,"output_type":"execute_result","data":{"text/plain":"tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,\n         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n         2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n         3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,\n         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n         5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n         6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n         7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,\n         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,\n         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n         9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n        10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n        11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13,\n        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n        13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n        14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n        15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17,\n        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18,\n        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n        19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n        19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n        19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n        20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22,\n        22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n        22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24,\n        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25,\n        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26,\n        26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n        26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n        27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28,\n        28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29,\n        29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n        29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n        30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31], device='cuda:0')"},"metadata":{}}]},{"cell_type":"markdown","source":"## Graph Prediction Model","metadata":{"id":"7WLhguSTeazy"}},{"cell_type":"markdown","source":"Now we will implement our GCN Graph Prediction model!\n\nWe will reuse the existing GCN model to generate `node_embeddings` and use  Global Pooling on the nodes to predict properties for the whole graph.","metadata":{"id":"u05Z14TRYPGn"}},{"cell_type":"code","source":"from ogb.graphproppred.mol_encoder import AtomEncoder\nfrom torch_geometric.nn import global_add_pool, global_mean_pool\n\n### GCN to predict graph property\nclass GCN_Graph(torch.nn.Module):\n    def __init__(self, hidden_dim, output_dim, num_layers, dropout):\n        super(GCN_Graph, self).__init__()\n\n        # Load encoders for Atoms in molecule graphs\n        # encodes x to (x,hidden_dim)\n        self.node_encoder = AtomEncoder(hidden_dim)\n\n        # Node embedding model\n        # Note that the input_dim and output_dim are set to hidden_dim\n        self.gnn_node = GCN(hidden_dim, hidden_dim,\n            hidden_dim, num_layers, dropout, return_embeds=True)\n\n#         self.pool = None\n\n        ############# Your code here ############\n        ## Note:\n        ## 1. Initialize the self.pool to global mean pooling layer\n        ## More information please refer to the documentation:\n        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#global-pooling-layers\n        ## (~1 line of code)\n        \n        self.pool = global_mean_pool\n\n        #########################################\n\n        # Output layer\n        self.linear = torch.nn.Linear(hidden_dim, output_dim)\n\n\n    def reset_parameters(self):\n        self.gnn_node.reset_parameters()\n        self.linear.reset_parameters()\n\n    def forward(self, batched_data):\n        # TODO: Implement this function that takes the input tensor batched_data,\n        # returns a batched output tensor for each graph.\n        # unwraps batched_data\n        x, edge_index, batch = batched_data.x, batched_data.edge_index, batched_data.batch\n        embed = self.node_encoder(x)\n        ############# Your code here ############\n        ## Note:\n        ## 1. Construct node embeddings using existing GCN model\n        ## 2. Use global pooling layer to construct features for the whole graph\n        ## More information please refer to the documentation:\n        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#global-pooling-layers\n        ## 3. Use a linear layer to predict the graph property \n        ## (~3 lines of code)\n        out = self.gnn_node(embed,edge_index)\n        out = self.pool(out,batch) # this layer pools the \n        out = self.linear(out)\n        \n\n        #########################################\n\n        return out","metadata":{"id":"3_Kq3zyjeZ22","trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"code","source":"def train(model, device, data_loader, optimizer, loss_fn):\n    \n    # TODO: Implement this function that trains the model by \n    # using the given optimizer and loss_fn.\n    model.train()\n#     loss = 0\n\n    for step, batch in enumerate(tqdm(data_loader, desc=\"Iteration\")):\n        batch = batch.to(device)\n        if batch.x.shape[0] == 1 or batch.batch[-1] == 0:\n            pass\n        else:\n            \n            ## ignore nan targets (unlabeled) when computing training loss.\n            is_labeled = batch.y == batch.y\n\n            ############# Your code here ############\n            ## Note:\n            ## 1. Zero grad the optimizer\n            ## 2. Feed the data into the model\n            ## 3. Use `is_labeled` mask to filter output and labels\n            ## 4. You might change the type of label\n            ## 5. Feed the output and label to loss_fn\n            ## (~3 lines of code)\n            optimizer.zero_grad()\n            y_pred = model(batch)\n            y_train = y_pred[is_labeled]\n            \n            # cast to float\n            loss = loss_fn(y_train,batch.y[is_labeled].squeeze().to(torch.float))\n\n            #########################################\n\n            loss.backward()\n            optimizer.step()\n\n    return loss.item()","metadata":{"id":"FJjnGuMSbjX0","trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"code","source":"# The evaluation function\ndef eval(model, device, loader, evaluator):\n    model.eval()\n    y_true = []\n    y_pred = []\n\n    for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n        batch = batch.to(device)\n\n        if batch.x.shape[0] == 1:\n            pass\n        else:\n            with torch.no_grad():\n                pred = model(batch)\n\n            y_true.append(batch.y.view(pred.shape).detach().cpu())\n            y_pred.append(pred.detach().cpu()) # send back to cpu because batch is in device gpu\n\n    y_true = torch.cat(y_true, dim = 0).numpy()\n    y_pred = torch.cat(y_pred, dim = 0).numpy()\n\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n\n    return evaluator.eval(input_dict)","metadata":{"id":"ztPHXq_Gzn7U","trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"model = GCN_Graph(args['hidden_dim'],\n            dataset.num_tasks, args['num_layers'],\n            args['dropout']).to(device)\nevaluator = Evaluator(name='ogbg-molhiv')","metadata":{"id":"MR1wQ4hMZeMw","trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"code","source":"import copy\n\nmodel.reset_parameters()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\nloss_fn = torch.nn.BCEWithLogitsLoss()\n\nbest_model = None\nbest_valid_acc = 0\n\nfor epoch in range(1, 1 + args[\"epochs\"]):\n    print('Training...')\n    loss = train(model, device, train_loader, optimizer, loss_fn)\n\n    print('Evaluating...')\n    # evaluator probably just evaluates different metrics and returns them in a dict\n    train_result = eval(model, device, train_loader, evaluator)\n    val_result = eval(model, device, valid_loader, evaluator)\n    test_result = eval(model, device, test_loader, evaluator)\n    \n    # dataset.eval_metric = 'rocauc'\n    # extract key rocauc\n    train_acc, valid_acc, test_acc = train_result[dataset.eval_metric], val_result[dataset.eval_metric], test_result[dataset.eval_metric]\n    if valid_acc > best_valid_acc:\n        best_valid_acc = valid_acc\n        best_model = copy.deepcopy(model)\n        print(f'Epoch: {epoch:02d}, '\n        f'Loss: {loss:.4f}, '\n        f'Train: {100 * train_acc:.2f}%, '\n        f'Valid: {100 * valid_acc:.2f}% '\n        f'Test: {100 * test_acc:.2f}%')","metadata":{"id":"qJGTNZiuZy0A","scrolled":true,"trusted":true},"execution_count":149,"outputs":[{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b513bab754704f7e839f79a3efade073"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d854feda0934dbaa1927d2423c3900c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b558109775c43ab98582e8238a7eba1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8484da40fa3c4640b392abe1f6e1bfc8"}},"metadata":{}},{"name":"stdout","text":"Epoch: 01, Loss: 0.0290, Train: 70.02%, Valid: 73.57% Test: 72.67%\nTraining...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"736e1f04fc3b4630977d8be9eb417e21"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c07c2b0b589472cabac7cbc02705cff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"962cc5f36a6644e8baebc4638566356e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48679b71c95047b896d3aa5f6976f5db"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f586562418a4077a0d54e78e633d77c"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a9c0c57baea44fd9fe97473e1efaa51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3aa39dc4903441a79998d59cdfa09bac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"925d4ab7c10647749538934ba6ffa23e"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2efdf168782141c69605f735064820f8"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee54678389544a28b69e9595560b3357"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"635b9b05ab8743508c56cd142231946f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"018f0c2d98f84026bb68d04df7bd5b16"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"896911f0bdd24981ab4eaacd96b32cf3"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d621503f3c6493aa7bae0a0b1762241"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ed182a877604eadb26f313d1280fbba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"902f61f52cd54518a41432850a6c0f8b"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caf0012163fc4321b5abbb5f8a6b128b"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb02e3016f314495a3aafc64779e0d34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c401e925c60146549e40920311a91897"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d67a13cd869f4ec6ab9bb2b7685d0f0b"}},"metadata":{}},{"name":"stdout","text":"Epoch: 06, Loss: 0.0187, Train: 76.96%, Valid: 77.96% Test: 73.85%\nTraining...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1c048e08d0d4d32bf350579134c7845"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"525da691710b494598a6bf1863a8a361"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16dabf5abc0447438eafb83377719ac7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92983985b5344e2195fae91a4160b310"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c182d2ee77f4488b87eebc8d342ddd83"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9e9298a1d174106aaefe02430eaa338"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c255b540fe14a5e9cadc6084536ec37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"451d97db6f2541ea97b160e64b4f29ea"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4c70a428f5e494b9336469a49db123d"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd9e5dc646684ff59ed026ad466aaecf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6623caa7ae0843e99bae1fc34d0cebc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da9c04f5748c45418ef7cf2fff685b77"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99d3d31953be4a7181e5f94ea905a8c1"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93dd4d961841404f846cd82f396518aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40aa53b34f7e4a4ab120588c0184e35f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0078d8b8160468fb57f7c42e059d1cb"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3e12dbe7dab4a6b9eb1230880fd10b0"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93cb27b99f794f0394d69ce96f32bb37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fec4a81fe724598a2bcad1fdfd533f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c55f3c257be04c66975650b183f32952"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a98938b73ed742458d41ea46e29fe1a4"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c81ef67c7cd34a918e95488b3a2f6a31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d88312855ae2429385d9d38cf1cf7574"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23eab8f7ab51429ca176427f00bbb199"}},"metadata":{}},{"name":"stdout","text":"Epoch: 12, Loss: 0.0381, Train: 79.12%, Valid: 78.45% Test: 73.02%\nTraining...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ae0c6e6eca9432c96de1950615b1340"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b10ac2ed9fa649df81046de7d83497cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b28fa21327844d6db2e89cbb72a32584"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"363927c72366408cbcb71a54a8cdae50"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bace8ebb37c548fcb9b2d9622c482d3e"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1386a324747e4f63834c93a30bd505cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdb33ae22cb04bb694431ad836a53bb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b0c9e1f0699475a9f5bf9307aa99589"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7062abb6a5644614b6c226fdd81b70aa"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b724b04e6529407e9df4200b4127c1b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30a2beb42de44e06b0c75e48fad2025d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9fd88583f6f4bc79abc4946d6437b04"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01ce7eea42a04a1897418c23ef143371"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1803cbb2087e43a18ce1472f37463f2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43edc07665fb45328f68f362e07f4235"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50ee17f4bcb342a29a7569f0ef6ba124"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11e370ea6c924e1c8262ba36398bbcea"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeea563943524d58b264f760f8afaf2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59cda16d53a24967b624af8eb89ad43e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44b2d71b23af472da329f8af249b2f1e"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7181420f5cee4b718902c97f5ec1edea"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4231e8d5e19748e5a010632a5780ec0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68b32a5651b643d0981f53cfdaa739f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ee21d102fb24cb0ba0b486f00d606f0"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c48a75e0486142c8957af0d32276bf2c"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"623c7d03ba2d48da8b62bae5e7fc9ff6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c08eacf91ba6449c93047bf0dfc787ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7876cab2d47b43afacc3d3fff348c252"}},"metadata":{}},{"name":"stdout","text":"Epoch: 19, Loss: 0.5390, Train: 80.30%, Valid: 78.62% Test: 73.57%\nTraining...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e132aa72a36e4ddea6d9480f6d1add05"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28d1db3d99504bd380c52dfe5ecea643"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94af960e4c4a4e48ae0dcd82fd15a656"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e48bc64b87db4b66b91be6940faa2a85"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"111f77e91d754a36adb50b591a701d97"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81819462be6746efb0b46a300c01d67a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"883f4d048a624be194495ddb769fb75e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aaf4552644c467a85222a3db3698fd3"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec4eeec36f5b4fcb9cd67152b5d42111"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00ade17c1a124598a4fa58652f644f1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"388f0ff0b9464b50bc771ace4b978822"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7020fcaa063b4315a4bcb2e304852882"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ed91e6108ae47d39de8b7385b117524"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21cafca898da4e30be0fbc972e266e8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3771a4d5627a4258a325c770cb56c938"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4eab5ed6fcc47a08020957f040ffc93"}},"metadata":{}},{"name":"stdout","text":"Epoch: 23, Loss: 0.1708, Train: 81.94%, Valid: 80.54% Test: 73.44%\nTraining...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4a83c9e03d6455a9342bdbc8194056f"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb4877e5c7b24d23866eac0c8175b329"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"243169970e144c018ae90adc5d297e83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92a5df49954e4649b1754b40210228ba"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d95f7ecd0a74f9486af8753c2e34b10"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ca64f2b514647818a697a35ee0dae6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4174124fb3f143888e13d4dbce847b1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c0009f49c2d46db966bcc9eb8b14a11"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31ec58c043694d0bb617f4e998ed1962"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5b79994d5c147cc9e39a3f6159ab85b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61ca14198f0341c0bceeef79c4d32f17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63869d94fb474ae0a2e74c232030ba3e"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bbd10e645524ea19d36b7d1dc2026f1"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a418e3c924464883a8f34d4aee70651c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe21ef3775aa4578853f65c2f6e8a4b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cd8c092f68641b58c6489efacafe832"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c36af4e85df1415a883fbcabac7fed30"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8505ce887e2c4bb2942dc3846ce2eef2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1abda9eb5c634894924b4762788efc8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78a79fab492840999fc193dbf6d718fa"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42999bd38e4f4668be2c9648477fa524"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6370a26005bd4b71930c67d8b784a7ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe1d9b345a7e49e18712578514da9b1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"051cb5d2116e466b8b937748a64b81bf"}},"metadata":{}},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a26128bfa4194c92bfc024224cc966f3"}},"metadata":{}},{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"071a6b9859d34419bc0493cbc2a408ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1b360b97a434904b564cfd95bd0d7d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e06f5722a4744adb02b56c6f8671e3e"}},"metadata":{}}]},{"cell_type":"code","source":"train_acc = eval(best_model, device, train_loader, evaluator)[dataset.eval_metric]\nvalid_acc = eval(best_model, device, valid_loader, evaluator)[dataset.eval_metric]\ntest_acc = eval(best_model, device, test_loader, evaluator)[dataset.eval_metric]\n\nprint(f'Best model: '\n      f'Train: {100 * train_acc:.2f}%, '\n      f'Valid: {100 * valid_acc:.2f}% '\n      f'Test: {100 * test_acc:.2f}%')","metadata":{"id":"Oq5QaG21dOOO","trusted":true},"execution_count":150,"outputs":[{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1029 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56d2cf9e2ff446e08fb4ae03c657c7d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c66ca6e12d14515b9f137bb1d585e66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/129 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5a5d29f5cbc4a65914b5a05ecf29aed"}},"metadata":{}},{"name":"stdout","text":"Best model: Train: 81.03%, Valid: 77.66% Test: 72.81%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Question 6: What are your `best_model` validation and test ROC-AUC score? Please report them on Gradescope. For example, for an ROC-AUC score such as 50.01%, just report 50.01 and please don't include the percent sign. (20 points)","metadata":{"id":"-uKs6j6t1ah3"}},{"cell_type":"markdown","source":"## Question 7 (Optional): Experiment with other two global pooling layers other than mean pooling in Pytorch Geometric.","metadata":{"id":"gBi_t8n0iZ4P"}},{"cell_type":"code","source":"","metadata":{"id":"taxEEWyh1_jq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n\nIn order to get credit, you must go submit your answers on Gradescope.\n\nAlso, you need to submit the `ipynb` file of Colab 2, by clicking `File` and `Download .ipynb`. Please make sure that your output of each cell is available in your `ipynb` file.","metadata":{"id":"e7JXsMTBgeOI"}}]}